{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix, ConfusionMatrixDisplay, auc, plot_roc_curve, f1_score, r2_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "plt.style.use('seaborn-bright')\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CUTOFF = '2020-01-31'\n",
    "TRAIN_VALID_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def _precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    " \n",
    "def _f1(y_true, y_pred):\n",
    "    precision = _precision(y_true, y_pred)\n",
    "    recall = _recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    " \n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = _f1(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = _f1(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 60, 1, 8)          600       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 29, 1, 8)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 104)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 104)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,105\n",
      "Trainable params: 1,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 18:54:18.322436: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-12 18:54:18.322563: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=74, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "cnnpred_2d().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(df, seq_len, batch_size, target_col, kind):\n",
    "    \"\"\"A generator to produce samples for Keras model\"\"\"\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Set up splitting parameters\n",
    "        input_cols = [c for c in df.columns if c != target_col]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "\n",
    "        # Range for the training set\n",
    "        if kind == 'train':\n",
    "            index = index[:split]\n",
    "\n",
    "        # Range for the validation set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   \n",
    "\n",
    "        while True:\n",
    "            \"Pick one position, then clip a sequence length\"\n",
    "\n",
    "            # Pick one time step\n",
    "            t = random.choice(index)\n",
    "\n",
    "            # Find its position in the DataFrame      \n",
    "            n = (df.index == t).argmax()\n",
    "\n",
    "            # Start over if there isn't enough data for one sequence length  \n",
    "            if (n - seq_len + 1) < 0:\n",
    "                continue\n",
    "            \n",
    "            # Create the DataFrame of one sequence length\n",
    "            frame = df.iloc[n - seq_len+1 : n+1]\n",
    "\n",
    "            # Append X and y values as a sample in the CNN dataset\n",
    "            batch.append([frame[input_cols].values, df.loc[t, target_col]])\n",
    "\n",
    "            break\n",
    "\n",
    "        # If we get enough for a batch, yield the instance\n",
    "        if len(batch) == batch_size:\n",
    "\n",
    "            # Unpack the `batch` list into features and target\n",
    "            X, y = zip(*batch)\n",
    "\n",
    "            # Expand dimensions of X\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "\n",
    "            # Yield the sample\n",
    "            yield X, y\n",
    "\n",
    "            # Clear the batch list for next iteration\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testgen(df, seq_len, target_col):\n",
    "    \"Return array of all test samples\"\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    input_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "    # find the start of test sample\n",
    "    t = df.index[df.index > TRAIN_TEST_CUTOFF][0]\n",
    "    n = (df.index == t).argmax()\n",
    "\n",
    "    for i in range(n+1, len(df)+1):\n",
    "\n",
    "        frame = df.iloc[i-seq_len:i]\n",
    "        batch.append([frame[input_cols].values, frame[target_col][-1]])\n",
    "\n",
    "    X, y = zip(*batch)\n",
    "\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../csv/initial_variables.csv', index_col='date', parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trend</th>\n",
       "      <th>rsi</th>\n",
       "      <th>rsi_fast_k</th>\n",
       "      <th>rsi_fast_d</th>\n",
       "      <th>williams_r</th>\n",
       "      <th>...</th>\n",
       "      <th>stk_wmt</th>\n",
       "      <th>stk_xom</th>\n",
       "      <th>usd_aud</th>\n",
       "      <th>usd_cad</th>\n",
       "      <th>usd_cny</th>\n",
       "      <th>usd_eur</th>\n",
       "      <th>usd_hkd</th>\n",
       "      <th>usd_jpy</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-01</th>\n",
       "      <td>-0.376407</td>\n",
       "      <td>-0.374811</td>\n",
       "      <td>-0.327073</td>\n",
       "      <td>-0.348369</td>\n",
       "      <td>0.319761</td>\n",
       "      <td>-0.574346</td>\n",
       "      <td>0.589285</td>\n",
       "      <td>0.394278</td>\n",
       "      <td>0.732668</td>\n",
       "      <td>0.737008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828320</td>\n",
       "      <td>0.812334</td>\n",
       "      <td>-0.614870</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.024215</td>\n",
       "      <td>0.080446</td>\n",
       "      <td>-1.970473</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>-0.378137</td>\n",
       "      <td>-0.378863</td>\n",
       "      <td>-0.334545</td>\n",
       "      <td>-0.352539</td>\n",
       "      <td>0.288836</td>\n",
       "      <td>-0.573524</td>\n",
       "      <td>0.534255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405188</td>\n",
       "      <td>0.571438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.821414</td>\n",
       "      <td>0.793945</td>\n",
       "      <td>-0.612061</td>\n",
       "      <td>-0.809202</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.051555</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>-1.949612</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.382135</td>\n",
       "      <td>-0.371074</td>\n",
       "      <td>-0.330230</td>\n",
       "      <td>-0.342036</td>\n",
       "      <td>0.233466</td>\n",
       "      <td>-0.572855</td>\n",
       "      <td>0.643092</td>\n",
       "      <td>0.937216</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>0.951483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813385</td>\n",
       "      <td>0.823759</td>\n",
       "      <td>-0.609970</td>\n",
       "      <td>-0.799731</td>\n",
       "      <td>0.028987</td>\n",
       "      <td>0.083581</td>\n",
       "      <td>0.090072</td>\n",
       "      <td>-1.962449</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.371566</td>\n",
       "      <td>-0.368835</td>\n",
       "      <td>-0.320004</td>\n",
       "      <td>-0.340727</td>\n",
       "      <td>0.154756</td>\n",
       "      <td>-0.572257</td>\n",
       "      <td>0.655480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.645739</td>\n",
       "      <td>0.923521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.817721</td>\n",
       "      <td>0.821973</td>\n",
       "      <td>-0.628568</td>\n",
       "      <td>-0.826070</td>\n",
       "      <td>0.033718</td>\n",
       "      <td>-0.044524</td>\n",
       "      <td>0.090843</td>\n",
       "      <td>-1.931157</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>-0.370154</td>\n",
       "      <td>-0.365761</td>\n",
       "      <td>-0.318638</td>\n",
       "      <td>-0.337850</td>\n",
       "      <td>0.224873</td>\n",
       "      <td>-0.571278</td>\n",
       "      <td>0.682726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979072</td>\n",
       "      <td>0.925917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.822379</td>\n",
       "      <td>0.830364</td>\n",
       "      <td>-0.629306</td>\n",
       "      <td>-0.825949</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>-0.024996</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>-1.964054</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.686622</td>\n",
       "      <td>0.676391</td>\n",
       "      <td>0.686857</td>\n",
       "      <td>0.680367</td>\n",
       "      <td>0.149923</td>\n",
       "      <td>0.625614</td>\n",
       "      <td>0.476807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329419</td>\n",
       "      <td>0.685404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442235</td>\n",
       "      <td>1.692497</td>\n",
       "      <td>0.379347</td>\n",
       "      <td>0.330339</td>\n",
       "      <td>0.492177</td>\n",
       "      <td>1.309952</td>\n",
       "      <td>1.915902</td>\n",
       "      <td>2.708257</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.685882</td>\n",
       "      <td>0.711731</td>\n",
       "      <td>0.703116</td>\n",
       "      <td>0.721812</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>0.629898</td>\n",
       "      <td>0.582036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421536</td>\n",
       "      <td>0.950153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534936</td>\n",
       "      <td>1.745130</td>\n",
       "      <td>0.380967</td>\n",
       "      <td>0.339688</td>\n",
       "      <td>0.507702</td>\n",
       "      <td>1.381893</td>\n",
       "      <td>1.918398</td>\n",
       "      <td>2.751505</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.716734</td>\n",
       "      <td>0.727854</td>\n",
       "      <td>0.719716</td>\n",
       "      <td>0.741540</td>\n",
       "      <td>0.249940</td>\n",
       "      <td>0.636176</td>\n",
       "      <td>0.626632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.982153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598480</td>\n",
       "      <td>1.774160</td>\n",
       "      <td>0.360919</td>\n",
       "      <td>0.316682</td>\n",
       "      <td>0.501759</td>\n",
       "      <td>1.321669</td>\n",
       "      <td>1.915324</td>\n",
       "      <td>2.682580</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>0.742009</td>\n",
       "      <td>0.752904</td>\n",
       "      <td>0.754181</td>\n",
       "      <td>0.764922</td>\n",
       "      <td>0.241794</td>\n",
       "      <td>0.643511</td>\n",
       "      <td>0.676221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644731</td>\n",
       "      <td>1.890550</td>\n",
       "      <td>0.358432</td>\n",
       "      <td>0.312576</td>\n",
       "      <td>0.487690</td>\n",
       "      <td>1.334401</td>\n",
       "      <td>1.923023</td>\n",
       "      <td>2.545054</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>0.752355</td>\n",
       "      <td>0.754869</td>\n",
       "      <td>0.760894</td>\n",
       "      <td>0.760210</td>\n",
       "      <td>0.207166</td>\n",
       "      <td>0.650708</td>\n",
       "      <td>0.661479</td>\n",
       "      <td>0.911954</td>\n",
       "      <td>0.970651</td>\n",
       "      <td>0.938111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654584</td>\n",
       "      <td>1.824080</td>\n",
       "      <td>0.370879</td>\n",
       "      <td>0.316072</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>1.322450</td>\n",
       "      <td>1.921867</td>\n",
       "      <td>2.465618</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2516 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close    volume     trend  \\\n",
       "date                                                                     \n",
       "2012-08-01 -0.376407 -0.374811 -0.327073 -0.348369  0.319761 -0.574346   \n",
       "2012-08-02 -0.378137 -0.378863 -0.334545 -0.352539  0.288836 -0.573524   \n",
       "2012-08-03 -0.382135 -0.371074 -0.330230 -0.342036  0.233466 -0.572855   \n",
       "2012-08-06 -0.371566 -0.368835 -0.320004 -0.340727  0.154756 -0.572257   \n",
       "2012-08-07 -0.370154 -0.365761 -0.318638 -0.337850  0.224873 -0.571278   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-07-26  0.686622  0.676391  0.686857  0.680367  0.149923  0.625614   \n",
       "2022-07-27  0.685882  0.711731  0.703116  0.721812  0.212572  0.629898   \n",
       "2022-07-28  0.716734  0.727854  0.719716  0.741540  0.249940  0.636176   \n",
       "2022-07-29  0.742009  0.752904  0.754181  0.764922  0.241794  0.643511   \n",
       "2022-08-01  0.752355  0.754869  0.760894  0.760210  0.207166  0.650708   \n",
       "\n",
       "                 rsi  rsi_fast_k  rsi_fast_d  williams_r  ...   stk_wmt  \\\n",
       "date                                                      ...             \n",
       "2012-08-01  0.589285    0.394278    0.732668    0.737008  ... -0.828320   \n",
       "2012-08-02  0.534255    0.000000    0.405188    0.571438  ... -0.821414   \n",
       "2012-08-03  0.643092    0.937216    0.443831    0.951483  ... -0.813385   \n",
       "2012-08-06  0.655480    1.000000    0.645739    0.923521  ... -0.817721   \n",
       "2012-08-07  0.682726    1.000000    0.979072    0.925917  ... -0.822379   \n",
       "...              ...         ...         ...         ...  ...       ...   \n",
       "2022-07-26  0.476807    0.000000    0.329419    0.685404  ...  0.442235   \n",
       "2022-07-27  0.582036    1.000000    0.421536    0.950153  ...  0.534936   \n",
       "2022-07-28  0.626632    1.000000    0.666667    0.982153  ...  0.598480   \n",
       "2022-07-29  0.676221    1.000000    1.000000    0.976827  ...  0.644731   \n",
       "2022-08-01  0.661479    0.911954    0.970651    0.938111  ...  0.654584   \n",
       "\n",
       "             stk_xom   usd_aud   usd_cad   usd_cny   usd_eur   usd_hkd  \\\n",
       "date                                                                     \n",
       "2012-08-01  0.812334 -0.614870 -0.815380  0.021953  0.024215  0.080446   \n",
       "2012-08-02  0.793945 -0.612061 -0.809202  0.030443  0.051555  0.104315   \n",
       "2012-08-03  0.823759 -0.609970 -0.799731  0.028987  0.083581  0.090072   \n",
       "2012-08-06  0.821973 -0.628568 -0.826070  0.033718 -0.044524  0.090843   \n",
       "2012-08-07  0.830364 -0.629306 -0.825949  0.036750 -0.024996  0.104315   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-07-26  1.692497  0.379347  0.330339  0.492177  1.309952  1.915902   \n",
       "2022-07-27  1.745130  0.380967  0.339688  0.507702  1.381893  1.918398   \n",
       "2022-07-28  1.774160  0.360919  0.316682  0.501759  1.321669  1.915324   \n",
       "2022-07-29  1.890550  0.358432  0.312576  0.487690  1.334401  1.923023   \n",
       "2022-08-01  1.824080  0.370879  0.316072  0.484536  1.322450  1.921867   \n",
       "\n",
       "             usd_jpy  day_of_week  target  \n",
       "date                                       \n",
       "2012-08-01 -1.970473         0.50       0  \n",
       "2012-08-02 -1.949612         0.75       1  \n",
       "2012-08-03 -1.962449         1.00       1  \n",
       "2012-08-06 -1.931157         0.00       1  \n",
       "2012-08-07 -1.964054         0.25       1  \n",
       "...              ...          ...     ...  \n",
       "2022-07-26  2.708257         0.25       1  \n",
       "2022-07-27  2.751505         0.50       1  \n",
       "2022-07-28  2.682580         0.75       1  \n",
       "2022-07-29  2.545054         1.00       0  \n",
       "2022-08-01  2.465618         0.00       0  \n",
       "\n",
       "[2516 rows x 75 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.columns\n",
    "\n",
    "# If the current price is higher than yesterday's price then target = 1, else 0\n",
    "data['target'] = (data['close'].pct_change().shift(-1) > 0).astype(int)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Fit the scaler using the training dataset\n",
    "index = data.index[data.index > TRAIN_TEST_CUTOFF]\n",
    "index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "scaler = MinMaxScaler().fit(data.loc[index, cols])\n",
    "\n",
    "# Save scale transformed dataframe\n",
    "data[cols] = scaler.transform(data[cols])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 60, 1, 8)          600       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 29, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 104)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 104)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,105\n",
      "Trainable params: 1,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 60\n",
    "batch_size = 64\n",
    "n_epochs = 20\n",
    "n_features = 74\n",
    " \n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['acc', f1macro])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './models/cp2d-{epoch}-{val_f1macro:.2f}.h5'\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode='max', verbose=0,\n",
    "                    save_best_only=True, save_weights_only=False, save_freq='epoch')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 18:54:18.803081: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-12 18:54:19.244220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/400 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.5458 - f1macro: 0.3519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 18:54:34.719133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 16s 39ms/step - loss: 0.4566 - acc: 0.5456 - f1macro: 0.3519 - val_loss: 0.4579 - val_acc: 0.5422 - val_f1macro: 0.3497\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 15s 38ms/step - loss: 0.4545 - acc: 0.5455 - f1macro: 0.3520 - val_loss: 0.4500 - val_acc: 0.5500 - val_f1macro: 0.3544\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.4539 - acc: 0.5461 - f1macro: 0.3522 - val_loss: 0.4547 - val_acc: 0.5453 - val_f1macro: 0.3524\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 16s 39ms/step - loss: 0.4505 - acc: 0.5495 - f1macro: 0.3535 - val_loss: 0.3875 - val_acc: 0.6125 - val_f1macro: 0.3792\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 16s 39ms/step - loss: 0.4495 - acc: 0.5505 - f1macro: 0.3541 - val_loss: 0.4531 - val_acc: 0.5469 - val_f1macro: 0.3529\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 16s 39ms/step - loss: 0.4566 - acc: 0.5434 - f1macro: 0.3511 - val_loss: 0.4188 - val_acc: 0.5813 - val_f1macro: 0.3667\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4502 - acc: 0.5498 - f1macro: 0.3537 - val_loss: 0.4250 - val_acc: 0.5750 - val_f1macro: 0.3643\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.4550 - acc: 0.5450 - f1macro: 0.3517 - val_loss: 0.4500 - val_acc: 0.5500 - val_f1macro: 0.3539\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.4581 - acc: 0.5419 - f1macro: 0.3505 - val_loss: 0.4469 - val_acc: 0.5531 - val_f1macro: 0.3547\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.4609 - acc: 0.5391 - f1macro: 0.3491 - val_loss: 0.4328 - val_acc: 0.5672 - val_f1macro: 0.3606\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.4557 - acc: 0.5443 - f1macro: 0.3514 - val_loss: 0.4250 - val_acc: 0.5750 - val_f1macro: 0.3643\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.4485 - acc: 0.5515 - f1macro: 0.3545 - val_loss: 0.4328 - val_acc: 0.5672 - val_f1macro: 0.3609\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.4540 - acc: 0.5460 - f1macro: 0.3522 - val_loss: 0.4547 - val_acc: 0.5453 - val_f1macro: 0.3521\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4497 - acc: 0.5503 - f1macro: 0.3540 - val_loss: 0.4422 - val_acc: 0.5578 - val_f1macro: 0.3569\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 17s 41ms/step - loss: 0.4516 - acc: 0.5484 - f1macro: 0.3530 - val_loss: 0.4547 - val_acc: 0.5453 - val_f1macro: 0.3509\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 17s 41ms/step - loss: 0.4537 - acc: 0.5462 - f1macro: 0.3523 - val_loss: 0.4359 - val_acc: 0.5641 - val_f1macro: 0.3595\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.4558 - acc: 0.5442 - f1macro: 0.3514 - val_loss: 0.4672 - val_acc: 0.5328 - val_f1macro: 0.3468\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4569 - acc: 0.5431 - f1macro: 0.3508 - val_loss: 0.4266 - val_acc: 0.5734 - val_f1macro: 0.3639\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4514 - acc: 0.5486 - f1macro: 0.3531 - val_loss: 0.4484 - val_acc: 0.5516 - val_f1macro: 0.3544\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.4584 - acc: 0.5416 - f1macro: 0.3503 - val_loss: 0.4422 - val_acc: 0.5578 - val_f1macro: 0.3575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c3c41180>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_gen = datagen(data, seq_len, batch_size, 'target', 'train')\n",
    "validation_gen = datagen(data, seq_len, batch_size, 'target', 'valid')\n",
    "\n",
    "model.fit(\n",
    "    training_gen,\n",
    "    validation_data=validation_gen,\n",
    "    epochs=n_epochs, \n",
    "    steps_per_epoch=400, \n",
    "    validation_steps=10, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 5ms/step\n",
      "accuracy: 0.5421303656597775\n",
      "MAE: 0.4578696343402226\n",
      "F1: 0.7030927835051547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 18:59:48.964767: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"target\")\n",
    " \n",
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_target, test_pred)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8445747800586509"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(test_target, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e134d545e645ca52080ee6eed13987f7920fe874666f42bfb56f6a4194d5976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
