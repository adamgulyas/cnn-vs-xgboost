{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from os.path import join\n",
    "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from pathlib2 import Path\n",
    "from tensorflow.keras import backend as K, callbacks\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_pos = precision(y_true, y_pred)\n",
    "    recall_pos = recall(y_true, y_pred)\n",
    "    precision_neg = precision((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    recall_neg = recall((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    f_posit = 2 * ((precision_pos * recall_pos) / (precision_pos + recall_pos + K.epsilon()))\n",
    "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
    "\n",
    "    return (f_posit + f_neg) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file_fir):\n",
    "#     print(file_fir)\n",
    "#     try:\n",
    "#         df_raw = pd.read_csv(file_fir, index_col='date', parse_dates=True, infer_datetime_format=True) # parse_dates=['Date'])\n",
    "#     except IOError:\n",
    "#         print(\"IO ERROR\")\n",
    "#     return df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_data_warehouse():\n",
    "    global number_of_stocks\n",
    "    # global samples_in_each_stock\n",
    "    global n_features\n",
    "    # global order_stocks\n",
    "\n",
    "\n",
    "    data = pd.read_csv('../csv/initial_variables.csv', index_col='date', parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "\n",
    "    # dynamically build target based on number of periods to predict\n",
    "    target = (data['close'][days_to_predict:] / data['close'][:-days_to_predict].values).astype(int)\n",
    "\n",
    "\n",
    "    data = data[:-days_to_predict]\n",
    "    target.index = data.index\n",
    "\n",
    "    data = data.ffill()\n",
    "    data['target'] = target\n",
    "    target = data['target']\n",
    "    # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
    "    del data['target']\n",
    "\n",
    "    n_features = data.shape[1]\n",
    "    # samples_in_each_stock = data.shape[0]\n",
    "\n",
    "    X_train = data[data.index < '2020-01-31']\n",
    "\n",
    "    X_train_tmp = scale(X_train)\n",
    "    y_train_tmp = target[target.index < '2020-01-31']\n",
    "\n",
    "    X_train = X_train_tmp[:int(0.75 * X_train_tmp.shape[0])]\n",
    "    y_train = y_train_tmp[:int(0.75 * y_train_tmp.shape[0])]\n",
    "\n",
    "    X_valid = scale(X_train_tmp[int(0.75 * X_train_tmp.shape[0]) - seq_len:])\n",
    "    y_valid = y_train_tmp[int(0.75 * y_train_tmp.shape[0]) - seq_len:]\n",
    "\n",
    "    data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
    "    data.index = target.index\n",
    "    \n",
    "    X_test = np.array(data[data.index >= '2020-01-31'])\n",
    "    y_test = np.array(target[target.index >= '2020-01-31'])\n",
    "\n",
    "    data_warehouse = [X_train, y_train, X_test, y_test, X_valid, y_valid]\n",
    "    # data_warehouse[df_name] = [X_train, y_train, np.array(X_test), np.array(y_test), X_valid, y_valid]\n",
    "\n",
    "    return data_warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniforge3/envs/cnn/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:239: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-1.78022919, -1.78339602, -1.77748429, ..., -0.77416684,\n",
       "         -2.99349553, -0.01702592],\n",
       "        [-1.78903669, -1.80414801, -1.81693303, ..., -0.74053201,\n",
       "         -2.96712807,  0.69654917],\n",
       "        [-1.80938473, -1.76425812, -1.79414989, ..., -0.7606017 ,\n",
       "         -2.98335414,  1.41012425],\n",
       "        ...,\n",
       "        [ 1.18995834,  1.18658268,  1.14495812, ...,  1.55376196,\n",
       "         -0.1396651 , -0.730601  ],\n",
       "        [ 1.15159577,  1.13461943,  1.11479125, ...,  1.54833073,\n",
       "         -0.11451395, -0.01702592],\n",
       "        [ 1.10999596,  1.10510541,  1.1086526 , ...,  1.57002979,\n",
       "         -0.15031406,  0.69654917]]),\n",
       " date\n",
       " 2012-08-01    0\n",
       " 2012-08-02    1\n",
       " 2012-08-03    1\n",
       " 2012-08-06    1\n",
       " 2012-08-07    1\n",
       "              ..\n",
       " 2018-03-09    0\n",
       " 2018-03-12    0\n",
       " 2018-03-13    0\n",
       " 2018-03-14    0\n",
       " 2018-03-15    1\n",
       " Name: target, Length: 1414, dtype: int64,\n",
       " array([[ 0.73771624,  0.71718308,  0.68264964, ..., -0.3885846 ,\n",
       "          0.01355631,  1.41291204],\n",
       "        [ 0.68432008,  0.70139801,  0.7068394 , ..., -0.43916557,\n",
       "         -0.03835782, -1.44530134],\n",
       "        [ 0.73574839,  0.74512755,  0.75866662, ..., -0.43050445,\n",
       "         -0.01368585, -0.73074799],\n",
       "        ...,\n",
       "        [ 1.50324527,  1.57771679,  1.5321166 , ...,  1.89612717,\n",
       "          2.90153988, -0.01619465],\n",
       "        [ 1.58871081,  1.62248038,  1.58001194, ...,  1.89166336,\n",
       "          2.81323308,  0.6983587 ],\n",
       "        [ 1.65873104,  1.69202943,  1.67945751, ...,  1.90284287,\n",
       "          2.63703358,  1.41291204]]),\n",
       " array([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0]),\n",
       " array([[-0.92075255, -0.9632466 , -0.81573835, ..., -1.21512697,\n",
       "          1.40063791, -1.44465934],\n",
       "        [-0.88127319, -0.96638805, -0.84512184, ..., -1.00753663,\n",
       "          1.33048787, -0.73237133],\n",
       "        [-0.90761216, -0.98671705, -0.87138514, ..., -0.80648477,\n",
       "          1.52718933, -0.02008331],\n",
       "        ...,\n",
       "        [ 2.39008874,  2.53844967,  2.40230686, ..., -3.09440698,\n",
       "         -0.39858648, -0.73237133],\n",
       "        [ 2.58841392,  2.5840275 ,  2.50821316, ..., -3.18508611,\n",
       "         -0.2981004 , -0.02008331],\n",
       "        [ 2.39648362,  2.53921948,  2.3431992 , ..., -3.31892965,\n",
       "         -0.36682929,  0.69220471]]),\n",
       " date\n",
       " 2017-12-18    0\n",
       " 2017-12-19    0\n",
       " 2017-12-20    1\n",
       " 2017-12-21    0\n",
       " 2017-12-22    0\n",
       "              ..\n",
       " 2020-01-24    0\n",
       " 2020-01-27    1\n",
       " 2020-01-28    0\n",
       " 2020-01-29    1\n",
       " 2020-01-30    0\n",
       " Name: target, Length: 532, dtype: int64]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = construct_data_warehouse()\n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the images (2D matrices) of data for each Sequence length (60 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(data, target, seq_len):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(data.shape[0] - seq_len + 1):\n",
    "        \n",
    "        X.append(data[i: i + seq_len])\n",
    "        y.append(target[i + seq_len - 1])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train, vaildation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence(data, seq_len):\n",
    "\n",
    "    # for key, value in data.items():\n",
    "\n",
    "    X_train, y_train = build_snapshots(\n",
    "        data[0], data[1], seq_len\n",
    "    )\n",
    "\n",
    "    X_test, y_test = build_snapshots(\n",
    "        data[2], data[3], seq_len\n",
    "    )\n",
    "\n",
    "    X_valid, y_valid = build_snapshots(\n",
    "        data[4], data[5], seq_len\n",
    "    )\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_valid = np.array(X_valid)\n",
    "    y_valid = np.array(y_valid)\n",
    "\n",
    "    X_train = X_train.reshape(\n",
    "        X_train.shape[0], X_train.shape[1], X_train.shape[2], 1\n",
    "    )\n",
    "\n",
    "    X_test = X_test.reshape(\n",
    "        X_test.shape[0], X_test.shape[1], X_test.shape[2], 1\n",
    "    )\n",
    "\n",
    "    X_valid = X_valid.reshape(\n",
    "        X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], 1\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_acc(model, test_data, test_target):\n",
    "    overall_results = model.predict(test_data)\n",
    "    test_pred = (overall_results > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "\n",
    "    # print(f'test pred: {test_pred}')\n",
    "    # print(f'test target: {test_target}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
    "                   f1_score(test_pred, test_target, average='macro')]\n",
    "\n",
    "    return acc_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(data_warehouse):\n",
    "# def train(data_warehouse, i):\n",
    "    seq_len = 60\n",
    "    epochs = 200\n",
    "    drop_rate = 0.1\n",
    "\n",
    "    global cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target\n",
    "\n",
    "    # if i == 1:\n",
    "    #     print('sequencing ...')\n",
    "    cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = cnn_data_sequence(\n",
    "        data_warehouse, seq_len)\n",
    "\n",
    "    my_file = Path(join(Base_dir, f'2D-models/best-{epochs}-{seq_len}-{n_filters}-{drop_rate}.h5'))\n",
    "    filepath = join(Base_dir, f'2D-models/best-{epochs}-{seq_len}-{n_filters}-{drop_rate}.h5')\n",
    "    \n",
    "    if my_file.is_file():\n",
    "        print('loading model')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(' fitting model to target')\n",
    "        model = Sequential()\n",
    "\n",
    "        # layer 1\n",
    "        model.add(\n",
    "            Conv2D(n_filters[0], (1, n_features), activation='relu', input_shape=(seq_len, n_features, 1))\n",
    "        )\n",
    "        \n",
    "        # layer 2\n",
    "        model.add(Conv2D(n_filters[1], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "        # layer 3\n",
    "        model.add(Conv2D(n_filters[2], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(drop_rate))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mae', metrics=['acc', f1])\n",
    "\n",
    "        best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n",
    "                                               save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "\n",
    "        model.fit(cnn_train_data, cnn_train_target, epochs=epochs, batch_size=128, verbose=1,\n",
    "                        validation_data=(cnn_valid_data, cnn_valid_target), callbacks=[best_model])\n",
    "    model = load_model(filepath, custom_objects={'f1': f1})\n",
    "\n",
    "    return model, seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence_pre_train(data, target, seque_len):\n",
    "    new_data = []\n",
    "    new_target = []\n",
    "    for index in range(data.shape[0] - seque_len + 1):\n",
    "        new_data.append(data[index: index + seque_len])\n",
    "        new_target.append(target[index + seque_len - 1])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_target = np.array(new_target)\n",
    "\n",
    "    new_data = new_data.reshape(new_data.shape[0], new_data.shape[1], new_data.shape[2], 1)\n",
    "\n",
    "    return new_data, new_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data_warehouse, model, seque_len, order_stocks, cnn_results):\n",
    "    for name in order_stocks:\n",
    "        value = data_warehouse[name]\n",
    "        # train_data, train_target = cnn_data_sequence_pre_train(value[0], value[1], seque_len)\n",
    "        test_data, test_target = cnn_data_sequence_pre_train(value[2], value[3], seque_len)\n",
    "        # valid_data, valid_target = cnn_data_sequence_pre_train(value[4], value[5], seque_len)\n",
    "\n",
    "        cnn_results.append(sklearn_acc(model, test_data, test_target)[2])\n",
    "\n",
    "    return cnn_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_ann(data_warehouse, order_stocks):\n",
    "    # cnn_results = []\n",
    "    # dnn_results = []\n",
    "    # iterate_no = 4\n",
    "    # for i in range(1, iterate_no):\n",
    "    K.clear_session()\n",
    "    # print(i)\n",
    "    model, seq_len = train(data_warehouse)\n",
    "    # model, seq_len = train(data_warehouse, i)\n",
    "    # cnn_results, dnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
    "    cnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
    "\n",
    "    cnn_results = np.array(cnn_results)\n",
    "    cnn_results = cnn_results.reshape(iterate_no - 1, len(order_stocks))\n",
    "    cnn_results = pd.DataFrame(cnn_results, columns=order_stocks)\n",
    "    cnn_results = cnn_results.append([cnn_results.mean(), cnn_results.max(), cnn_results.std()], ignore_index=True)\n",
    "    cnn_results.to_csv(join(Base_dir, '2D-models/new results.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniforge3/envs/cnn/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:239: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_cnn_ann' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m data_warehouse \u001b[39m=\u001b[39m costruct_data_warehouse()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# data_warehouse = costruct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# order_stocks = data_warehouse.keys()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print('number of stocks = '), number_of_stocks\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adam/school/cnn-lstm/notebooks/cnn-test.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m run_cnn_ann(data_warehouse, order_stocks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_cnn_ann' is not defined"
     ]
    }
   ],
   "source": [
    "# Base_dir = '../'\n",
    "# TRAIN_ROOT_PATH = join(Base_dir, 'Dataset')\n",
    "# train_file_names = os.listdir(join(Base_dir, 'Dataset'))\n",
    "\n",
    "# if moving average = 0 then we have no moving average\n",
    "seq_len = 60\n",
    "moving_average_day = 0\n",
    "number_of_stocks = 0\n",
    "n_features = 0\n",
    "# samples_in_each_stock = 0\n",
    "n_filters = [8, 8, 8]\n",
    "days_to_predict = 1\n",
    "\n",
    "cnn_train_data = []\n",
    "cnn_train_target = []\n",
    "cnn_test_data = []\n",
    "cnn_test_target = []\n",
    "cnn_valid_data = []\n",
    "cnn_valid_target = []\n",
    "\n",
    "print('Loading train data ...')\n",
    "# order_stocks = []\n",
    "data_warehouse = costruct_data_warehouse()\n",
    "# data_warehouse = costruct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\n",
    "# order_stocks = data_warehouse.keys()\n",
    "\n",
    "# print('number of stocks = '), number_of_stocks\n",
    "\n",
    "run_cnn_ann(data_warehouse, order_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e134d545e645ca52080ee6eed13987f7920fe874666f42bfb56f6a4194d5976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
