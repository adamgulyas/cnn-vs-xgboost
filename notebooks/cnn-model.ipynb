{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from os.path import join\n",
    "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from pathlib2 import Path\n",
    "from tensorflow.keras import backend as K, callbacks\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_pos = precision(y_true, y_pred)\n",
    "    recall_pos = recall(y_true, y_pred)\n",
    "    precision_neg = precision((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    recall_neg = recall((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    f_posit = 2 * ((precision_pos * recall_pos) / (precision_pos + recall_pos + K.epsilon()))\n",
    "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
    "\n",
    "    return (f_posit + f_neg) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_dir = '../'\n",
    "seq_len = 60\n",
    "n_filters = [8, 8, 8]\n",
    "predict_period = 1\n",
    "epochs = 200\n",
    "drop_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "\n",
    "    data = pd.read_csv('../csv/initial_variables.csv', index_col='date', parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "    # dynamically build target based on number of periods to predict\n",
    "    target = (data['close'][predict_period:] / data['close'][:-predict_period].values).astype(int)\n",
    "\n",
    "    data = data[:-predict_period]\n",
    "    target.index = data.index\n",
    "    data = data.ffill()\n",
    "    data['target'] = target\n",
    "    target = data['target']\n",
    "    del data['target']\n",
    "\n",
    "    global n_features\n",
    "    n_features = data.shape[1]\n",
    "\n",
    "    X_train = data[data.index < '2020-01-31']\n",
    "\n",
    "    X_train_tmp = scale(X_train)\n",
    "    y_train_tmp = target[target.index < '2020-01-31']\n",
    "\n",
    "    X_train = X_train_tmp[:int(0.75 * X_train_tmp.shape[0])]\n",
    "    y_train = y_train_tmp[:int(0.75 * y_train_tmp.shape[0])]\n",
    "\n",
    "    X_valid = scale(X_train_tmp[int(0.75 * X_train_tmp.shape[0]) - seq_len:])\n",
    "    y_valid = y_train_tmp[int(0.75 * y_train_tmp.shape[0]) - seq_len:]\n",
    "\n",
    "    data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
    "    data.index = target.index\n",
    "    \n",
    "    X_test = data[data.index >= '2020-01-31']\n",
    "    y_test = target[target.index >= '2020-01-31']\n",
    "\n",
    "    dataset = [X_train, y_train, np.array(X_test), np.array(y_test), X_valid, y_valid]\n",
    "\n",
    "    return dataset, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the images (2D matrices) of data for each Sequence length (60 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(data, target, seq_len):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(data.shape[0] - seq_len + 1):\n",
    "        \n",
    "        X.append(data[i: i + seq_len])\n",
    "        y.append(target[i + seq_len - 1])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train, vaildation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence(data, seq_len):\n",
    "\n",
    "    X_train, y_train = build_snapshots(\n",
    "        data[0], data[1], seq_len\n",
    "    )\n",
    "\n",
    "    X_test, y_test = build_snapshots(\n",
    "        data[2], data[3], seq_len\n",
    "    )\n",
    "\n",
    "    X_valid, y_valid = build_snapshots(\n",
    "        data[4], data[5], seq_len\n",
    "    )\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_valid = np.array(X_valid)\n",
    "    y_valid = np.array(y_valid)\n",
    "\n",
    "    X_train = X_train.reshape(\n",
    "        X_train.shape[0], X_train.shape[1], X_train.shape[2], 1\n",
    "    )\n",
    "\n",
    "    X_test = X_test.reshape(\n",
    "        X_test.shape[0], X_test.shape[1], X_test.shape[2], 1\n",
    "    )\n",
    "\n",
    "    X_valid = X_valid.reshape(\n",
    "        X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], 1\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_acc(model, test_data, test_target):\n",
    "\n",
    "    global test_pred\n",
    "    \n",
    "    overall_results = model.predict(test_data)\n",
    "    test_pred = (overall_results > 0.5).astype(int)\n",
    "\n",
    "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
    "                   f1_score(test_pred, test_target, average='macro')]\n",
    "\n",
    "    return acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = cnn_data_sequence(\n",
    "        data, seq_len\n",
    "    )\n",
    "\n",
    "    my_file = Path(join(Base_dir, f'models/best-{epochs}-{seq_len}-{drop_rate}.h5'))\n",
    "    filepath = join(Base_dir, f'models/best-{epochs}-{seq_len}-{drop_rate}.h5')\n",
    "    \n",
    "    if my_file.is_file():\n",
    "        print('Loading model...')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(' Fitting model...')\n",
    "        model = Sequential()\n",
    "        # layer 1\n",
    "        model.add(Conv2D(n_filters[0], (1, n_features), activation='relu', input_shape=(seq_len, n_features, 1)))\n",
    "        # layer 2\n",
    "        model.add(Conv2D(n_filters[1], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "        # layer 3\n",
    "        model.add(Conv2D(n_filters[2], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(drop_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Build the model\n",
    "        model.compile(optimizer='adam', loss='mae', metrics=['acc', f1])\n",
    "\n",
    "        # Save the best model out of all trials\n",
    "        best_model = callbacks.ModelCheckpoint(\n",
    "            filepath, \n",
    "            monitor='val_f1', \n",
    "            verbose=0, \n",
    "            save_best_only=True,\n",
    "            ave_weights_only=False, \n",
    "            mode='max', \n",
    "            period=1\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs, \n",
    "            batch_size=128, \n",
    "            verbose=1,\n",
    "            validation_data=(X_valid, y_valid), \n",
    "            callbacks=[best_model]\n",
    "        )\n",
    "        \n",
    "    model = load_model(filepath, custom_objects={'f1': f1})\n",
    "\n",
    "    return model, seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence_pre_train(data, target, seq_len):\n",
    "\n",
    "    new_data = []\n",
    "    new_target = []\n",
    "    \n",
    "    for index in range(data.shape[0] - seq_len + 1):\n",
    "        \n",
    "        new_data.append(data[index: index + seq_len])\n",
    "        new_target.append(target[index + seq_len - 1])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_target = np.array(new_target)\n",
    "\n",
    "    new_data = new_data.reshape(new_data.shape[0], new_data.shape[1], new_data.shape[2], 1)\n",
    "\n",
    "    return new_data, new_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, model, seq_len):\n",
    "\n",
    "    X_test, y_test = cnn_data_sequence_pre_train(data[2], data[3], seq_len)\n",
    "\n",
    "    cnn_results = [sklearn_acc(model, X_test, y_test)[2]]\n",
    "\n",
    "    return cnn_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_ann(data):\n",
    "    \n",
    "    K.clear_session()\n",
    "    model, seq_len = train(data)\n",
    "    \n",
    "    results = prediction(data, model, seq_len)\n",
    "    results = np.array(results)\n",
    "    results = pd.DataFrame(results, columns=['SP500'])\n",
    "    results.to_csv(join(Base_dir, 'models/new results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data ...\n",
      "Loading model...\n",
      " 1/18 [>.............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniforge3/envs/cnn/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:239: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "2022-08-08 19:48:35.797060: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Loading train data ...')\n",
    "dataset, y_test = build_dataset()\n",
    "run_cnn_ann(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-01-31    1\n",
       "2020-02-03    1\n",
       "2020-02-04    1\n",
       "2020-02-05    1\n",
       "2020-02-06    0\n",
       "             ..\n",
       "2022-07-25    0\n",
       "2022-07-26    1\n",
       "2022-07-27    1\n",
       "2022-07-28    1\n",
       "2022-07-29    0\n",
       "Name: target, Length: 629, dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e134d545e645ca52080ee6eed13987f7920fe874666f42bfb56f6a4194d5976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
