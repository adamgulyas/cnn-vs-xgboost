{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CUTOFF = '2020-01-31'\n",
    "TRAIN_VALID_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def _precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    " \n",
    "def _f1(y_true, y_pred):\n",
    "    precision = _precision(y_true, y_pred)\n",
    "    recall = _recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    " \n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = _f1(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = _f1(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_63 (Conv2D)          (None, 60, 1, 8)          600       \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 29, 1, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPoolin  (None, 13, 1, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_21 (Flatten)        (None, 104)               0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 104)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,105\n",
      "Trainable params: 1,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=74, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "cnnpred_2d().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(df, seq_len, batch_size, target_col, kind):\n",
    "    \"\"\"A generator to produce samples for Keras model\"\"\"\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Set up splitting parameters\n",
    "        input_cols = [c for c in df.columns if c != target_col]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "\n",
    "        # Range for the training set\n",
    "        if kind == 'train':\n",
    "            index = index[:split]\n",
    "\n",
    "        # Range for the validation set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   \n",
    "\n",
    "        while True:\n",
    "            \"Pick one position, then clip a sequence length\"\n",
    "\n",
    "            # Pick one time step\n",
    "            t = random.choice(index)\n",
    "\n",
    "            # Find its position in the DataFrame      \n",
    "            n = (df.index == t).argmax()\n",
    "\n",
    "            # Start over if there isn't enough data for one sequence length  \n",
    "            if (n - seq_len + 1) < 0:\n",
    "                continue\n",
    "            \n",
    "            # Create the DataFrame of one sequence length\n",
    "            frame = df.iloc[n - seq_len+1 : n+1]\n",
    "\n",
    "            # Append X and y values as a sample in the CNN dataset\n",
    "            batch.append([frame[input_cols].values, df.loc[t, target_col]])\n",
    "\n",
    "            break\n",
    "\n",
    "        # If we get enough for a batch, yield the instance\n",
    "        if len(batch) == batch_size:\n",
    "\n",
    "            # Unpack the `batch` list into features and target\n",
    "            X, y = zip(*batch)\n",
    "\n",
    "            # Expand dimensions of X\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "\n",
    "            # Yield the sample\n",
    "            yield X, y\n",
    "\n",
    "            # Clear the batch list for next iteration\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testgen(df, seq_len, target_col):\n",
    "    \"Return array of all test samples\"\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    input_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "    # find the start of test sample\n",
    "    t = df.index[df.index > TRAIN_TEST_CUTOFF][0]\n",
    "    n = (df.index == t).argmax()\n",
    "\n",
    "    for i in range(n+1, len(df)+1):\n",
    "\n",
    "        frame = df.iloc[i-seq_len:i]\n",
    "        batch.append([frame[input_cols].values, frame[target_col][-1]])\n",
    "\n",
    "    X, y = zip(*batch)\n",
    "\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../csv/initial_variables.csv', index_col='date', parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trend</th>\n",
       "      <th>rsi</th>\n",
       "      <th>rsi_fast_k</th>\n",
       "      <th>rsi_fast_d</th>\n",
       "      <th>williams_r</th>\n",
       "      <th>...</th>\n",
       "      <th>stk_wmt</th>\n",
       "      <th>stk_xom</th>\n",
       "      <th>usd_aud</th>\n",
       "      <th>usd_cad</th>\n",
       "      <th>usd_cny</th>\n",
       "      <th>usd_eur</th>\n",
       "      <th>usd_hkd</th>\n",
       "      <th>usd_jpy</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-01</th>\n",
       "      <td>-3.927738</td>\n",
       "      <td>-3.992075</td>\n",
       "      <td>-3.849111</td>\n",
       "      <td>-3.929754</td>\n",
       "      <td>-0.000858</td>\n",
       "      <td>-4.067769</td>\n",
       "      <td>-0.125040</td>\n",
       "      <td>-0.487452</td>\n",
       "      <td>0.438703</td>\n",
       "      <td>0.116703</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.675313</td>\n",
       "      <td>1.040767</td>\n",
       "      <td>-4.615527</td>\n",
       "      <td>-5.219167</td>\n",
       "      <td>-1.191103</td>\n",
       "      <td>-1.437705</td>\n",
       "      <td>-0.654210</td>\n",
       "      <td>-10.338144</td>\n",
       "      <td>-0.003038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>-3.934715</td>\n",
       "      <td>-4.008711</td>\n",
       "      <td>-3.879867</td>\n",
       "      <td>-3.946913</td>\n",
       "      <td>-0.180844</td>\n",
       "      <td>-4.064781</td>\n",
       "      <td>-0.465806</td>\n",
       "      <td>-1.451297</td>\n",
       "      <td>-0.592878</td>\n",
       "      <td>-0.453170</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.644511</td>\n",
       "      <td>0.974987</td>\n",
       "      <td>-4.601028</td>\n",
       "      <td>-5.192207</td>\n",
       "      <td>-1.164388</td>\n",
       "      <td>-1.327174</td>\n",
       "      <td>-0.564953</td>\n",
       "      <td>-10.248450</td>\n",
       "      <td>0.712476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-3.950836</td>\n",
       "      <td>-3.976733</td>\n",
       "      <td>-3.862104</td>\n",
       "      <td>-3.903700</td>\n",
       "      <td>-0.503106</td>\n",
       "      <td>-4.062349</td>\n",
       "      <td>0.208150</td>\n",
       "      <td>0.839803</td>\n",
       "      <td>-0.471148</td>\n",
       "      <td>0.854900</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.608700</td>\n",
       "      <td>1.081638</td>\n",
       "      <td>-4.590233</td>\n",
       "      <td>-5.150877</td>\n",
       "      <td>-1.168968</td>\n",
       "      <td>-1.197698</td>\n",
       "      <td>-0.618212</td>\n",
       "      <td>-10.303646</td>\n",
       "      <td>1.427991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-3.908220</td>\n",
       "      <td>-3.967541</td>\n",
       "      <td>-3.820016</td>\n",
       "      <td>-3.898313</td>\n",
       "      <td>-0.961206</td>\n",
       "      <td>-4.060179</td>\n",
       "      <td>0.284862</td>\n",
       "      <td>0.993284</td>\n",
       "      <td>0.164870</td>\n",
       "      <td>0.758658</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.628040</td>\n",
       "      <td>1.075251</td>\n",
       "      <td>-4.686221</td>\n",
       "      <td>-5.265817</td>\n",
       "      <td>-1.154084</td>\n",
       "      <td>-1.715607</td>\n",
       "      <td>-0.615329</td>\n",
       "      <td>-10.169103</td>\n",
       "      <td>-1.434068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>-3.902525</td>\n",
       "      <td>-3.954921</td>\n",
       "      <td>-3.814391</td>\n",
       "      <td>-3.886474</td>\n",
       "      <td>-0.553116</td>\n",
       "      <td>-4.056620</td>\n",
       "      <td>0.453579</td>\n",
       "      <td>0.993284</td>\n",
       "      <td>1.214888</td>\n",
       "      <td>0.766904</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.648812</td>\n",
       "      <td>1.105268</td>\n",
       "      <td>-4.690031</td>\n",
       "      <td>-5.265287</td>\n",
       "      <td>-1.144542</td>\n",
       "      <td>-1.636657</td>\n",
       "      <td>-0.564953</td>\n",
       "      <td>-10.310547</td>\n",
       "      <td>-0.718553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.358625</td>\n",
       "      <td>0.323492</td>\n",
       "      <td>0.324125</td>\n",
       "      <td>0.303007</td>\n",
       "      <td>-0.989335</td>\n",
       "      <td>0.292450</td>\n",
       "      <td>-0.821548</td>\n",
       "      <td>-1.451297</td>\n",
       "      <td>-0.831555</td>\n",
       "      <td>-0.060913</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008615</td>\n",
       "      <td>4.189357</td>\n",
       "      <td>0.515773</td>\n",
       "      <td>-0.219405</td>\n",
       "      <td>0.288545</td>\n",
       "      <td>3.760341</td>\n",
       "      <td>6.209249</td>\n",
       "      <td>9.778225</td>\n",
       "      <td>-0.718553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.355644</td>\n",
       "      <td>0.468577</td>\n",
       "      <td>0.391047</td>\n",
       "      <td>0.473532</td>\n",
       "      <td>-0.624709</td>\n",
       "      <td>0.308015</td>\n",
       "      <td>-0.169930</td>\n",
       "      <td>0.993284</td>\n",
       "      <td>-0.541380</td>\n",
       "      <td>0.850320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595164</td>\n",
       "      <td>4.377641</td>\n",
       "      <td>0.524133</td>\n",
       "      <td>-0.178608</td>\n",
       "      <td>0.337396</td>\n",
       "      <td>4.051191</td>\n",
       "      <td>6.218583</td>\n",
       "      <td>9.964172</td>\n",
       "      <td>-0.003038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.480043</td>\n",
       "      <td>0.534768</td>\n",
       "      <td>0.459368</td>\n",
       "      <td>0.554705</td>\n",
       "      <td>-0.407222</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>0.106226</td>\n",
       "      <td>0.993284</td>\n",
       "      <td>0.230795</td>\n",
       "      <td>0.960463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311756</td>\n",
       "      <td>4.481488</td>\n",
       "      <td>0.420663</td>\n",
       "      <td>-0.279003</td>\n",
       "      <td>0.318696</td>\n",
       "      <td>3.807711</td>\n",
       "      <td>6.207087</td>\n",
       "      <td>9.667828</td>\n",
       "      <td>0.712476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>0.581961</td>\n",
       "      <td>0.637608</td>\n",
       "      <td>0.601223</td>\n",
       "      <td>0.650908</td>\n",
       "      <td>-0.454632</td>\n",
       "      <td>0.357480</td>\n",
       "      <td>0.413294</td>\n",
       "      <td>0.993284</td>\n",
       "      <td>1.280813</td>\n",
       "      <td>0.942131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105479</td>\n",
       "      <td>4.897848</td>\n",
       "      <td>0.407827</td>\n",
       "      <td>-0.296919</td>\n",
       "      <td>0.274424</td>\n",
       "      <td>3.859186</td>\n",
       "      <td>6.235878</td>\n",
       "      <td>9.076529</td>\n",
       "      <td>1.427991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>0.623677</td>\n",
       "      <td>0.645674</td>\n",
       "      <td>0.628854</td>\n",
       "      <td>0.631521</td>\n",
       "      <td>-0.656173</td>\n",
       "      <td>0.383632</td>\n",
       "      <td>0.322010</td>\n",
       "      <td>0.778048</td>\n",
       "      <td>1.188363</td>\n",
       "      <td>0.808875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061533</td>\n",
       "      <td>4.660067</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>-0.281665</td>\n",
       "      <td>0.264501</td>\n",
       "      <td>3.810869</td>\n",
       "      <td>6.231555</td>\n",
       "      <td>8.734993</td>\n",
       "      <td>-1.434068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2516 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close    volume     trend  \\\n",
       "date                                                                     \n",
       "2012-08-01 -3.927738 -3.992075 -3.849111 -3.929754 -0.000858 -4.067769   \n",
       "2012-08-02 -3.934715 -4.008711 -3.879867 -3.946913 -0.180844 -4.064781   \n",
       "2012-08-03 -3.950836 -3.976733 -3.862104 -3.903700 -0.503106 -4.062349   \n",
       "2012-08-06 -3.908220 -3.967541 -3.820016 -3.898313 -0.961206 -4.060179   \n",
       "2012-08-07 -3.902525 -3.954921 -3.814391 -3.886474 -0.553116 -4.056620   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-07-26  0.358625  0.323492  0.324125  0.303007 -0.989335  0.292450   \n",
       "2022-07-27  0.355644  0.468577  0.391047  0.473532 -0.624709  0.308015   \n",
       "2022-07-28  0.480043  0.534768  0.459368  0.554705 -0.407222  0.330827   \n",
       "2022-07-29  0.581961  0.637608  0.601223  0.650908 -0.454632  0.357480   \n",
       "2022-08-01  0.623677  0.645674  0.628854  0.631521 -0.656173  0.383632   \n",
       "\n",
       "                 rsi  rsi_fast_k  rsi_fast_d  williams_r  ...   stk_wmt  \\\n",
       "date                                                      ...             \n",
       "2012-08-01 -0.125040   -0.487452    0.438703    0.116703  ... -6.675313   \n",
       "2012-08-02 -0.465806   -1.451297   -0.592878   -0.453170  ... -6.644511   \n",
       "2012-08-03  0.208150    0.839803   -0.471148    0.854900  ... -6.608700   \n",
       "2012-08-06  0.284862    0.993284    0.164870    0.758658  ... -6.628040   \n",
       "2012-08-07  0.453579    0.993284    1.214888    0.766904  ... -6.648812   \n",
       "...              ...         ...         ...         ...  ...       ...   \n",
       "2022-07-26 -0.821548   -1.451297   -0.831555   -0.060913  ... -1.008615   \n",
       "2022-07-27 -0.169930    0.993284   -0.541380    0.850320  ... -0.595164   \n",
       "2022-07-28  0.106226    0.993284    0.230795    0.960463  ... -0.311756   \n",
       "2022-07-29  0.413294    0.993284    1.280813    0.942131  ... -0.105479   \n",
       "2022-08-01  0.322010    0.778048    1.188363    0.808875  ... -0.061533   \n",
       "\n",
       "             stk_xom   usd_aud   usd_cad   usd_cny   usd_eur   usd_hkd  \\\n",
       "date                                                                     \n",
       "2012-08-01  1.040767 -4.615527 -5.219167 -1.191103 -1.437705 -0.654210   \n",
       "2012-08-02  0.974987 -4.601028 -5.192207 -1.164388 -1.327174 -0.564953   \n",
       "2012-08-03  1.081638 -4.590233 -5.150877 -1.168968 -1.197698 -0.618212   \n",
       "2012-08-06  1.075251 -4.686221 -5.265817 -1.154084 -1.715607 -0.615329   \n",
       "2012-08-07  1.105268 -4.690031 -5.265287 -1.144542 -1.636657 -0.564953   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-07-26  4.189357  0.515773 -0.219405  0.288545  3.760341  6.209249   \n",
       "2022-07-27  4.377641  0.524133 -0.178608  0.337396  4.051191  6.218583   \n",
       "2022-07-28  4.481488  0.420663 -0.279003  0.318696  3.807711  6.207087   \n",
       "2022-07-29  4.897848  0.407827 -0.296919  0.274424  3.859186  6.235878   \n",
       "2022-08-01  4.660067  0.472066 -0.281665  0.264501  3.810869  6.231555   \n",
       "\n",
       "              usd_jpy  day_of_week  target  \n",
       "date                                        \n",
       "2012-08-01 -10.338144    -0.003038       0  \n",
       "2012-08-02 -10.248450     0.712476       1  \n",
       "2012-08-03 -10.303646     1.427991       1  \n",
       "2012-08-06 -10.169103    -1.434068       1  \n",
       "2012-08-07 -10.310547    -0.718553       1  \n",
       "...               ...          ...     ...  \n",
       "2022-07-26   9.778225    -0.718553       1  \n",
       "2022-07-27   9.964172    -0.003038       1  \n",
       "2022-07-28   9.667828     0.712476       1  \n",
       "2022-07-29   9.076529     1.427991       0  \n",
       "2022-08-01   8.734993    -1.434068       0  \n",
       "\n",
       "[2516 rows x 75 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.columns\n",
    "\n",
    "# If the current price is higher than yesterday's price then target = 1, else 0\n",
    "data['target'] = (data['close'].pct_change().shift(-1) > 0).astype(int)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Fit the standard scaler using the training dataset\n",
    "index = data.index[data.index > TRAIN_TEST_CUTOFF]\n",
    "index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "scaler = StandardScaler().fit(data.loc[index, cols])\n",
    "\n",
    "# Save scale transformed dataframe\n",
    "data[cols] = scaler.transform(data[cols])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_66 (Conv2D)          (None, 60, 1, 8)          600       \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_44 (MaxPoolin  (None, 29, 1, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_45 (MaxPoolin  (None, 13, 1, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_22 (Flatten)        (None, 104)               0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 104)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,105\n",
      "Trainable params: 1,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 60\n",
    "batch_size = 64\n",
    "n_epochs = 20\n",
    "n_features = 74\n",
    " \n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['acc', f1macro])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './models/cp2d-{epoch}-{val_f1macro:.2f}.h5'\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode='max', verbose=0,\n",
    "                    save_best_only=True, save_weights_only=False, save_freq='epoch')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 16:50:13.093621: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 0.4554 - acc: 0.5462 - f1macro: 0.3527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 16:50:21.078525: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 9s 41ms/step - loss: 0.4554 - acc: 0.5462 - f1macro: 0.3527 - val_loss: 0.4237 - val_acc: 0.5766 - val_f1macro: 0.3646\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.4565 - acc: 0.5435 - f1macro: 0.3510 - val_loss: 0.4313 - val_acc: 0.5688 - val_f1macro: 0.3609\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4535 - acc: 0.5465 - f1macro: 0.3523 - val_loss: 0.4563 - val_acc: 0.5437 - val_f1macro: 0.3505\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.4582 - acc: 0.5418 - f1macro: 0.3503 - val_loss: 0.4438 - val_acc: 0.5563 - val_f1macro: 0.3567\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.4513 - acc: 0.5487 - f1macro: 0.3535 - val_loss: 0.4391 - val_acc: 0.5609 - val_f1macro: 0.3582\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.4588 - acc: 0.5412 - f1macro: 0.3500 - val_loss: 0.4422 - val_acc: 0.5578 - val_f1macro: 0.3572\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4607 - acc: 0.5393 - f1macro: 0.3492 - val_loss: 0.4484 - val_acc: 0.5516 - val_f1macro: 0.3544\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4595 - acc: 0.5405 - f1macro: 0.3497 - val_loss: 0.4391 - val_acc: 0.5609 - val_f1macro: 0.3577\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4577 - acc: 0.5423 - f1macro: 0.3506 - val_loss: 0.4360 - val_acc: 0.5641 - val_f1macro: 0.3595\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4551 - acc: 0.5449 - f1macro: 0.3516 - val_loss: 0.4672 - val_acc: 0.5328 - val_f1macro: 0.3464\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 0.4514 - acc: 0.5486 - f1macro: 0.3532 - val_loss: 0.4500 - val_acc: 0.5500 - val_f1macro: 0.3543\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4573 - acc: 0.5427 - f1macro: 0.3504 - val_loss: 0.4406 - val_acc: 0.5594 - val_f1macro: 0.3582\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4577 - acc: 0.5423 - f1macro: 0.3505 - val_loss: 0.4609 - val_acc: 0.5391 - val_f1macro: 0.3495\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4542 - acc: 0.5458 - f1macro: 0.3522 - val_loss: 0.4516 - val_acc: 0.5484 - val_f1macro: 0.3532\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4551 - acc: 0.5449 - f1macro: 0.3520 - val_loss: 0.4141 - val_acc: 0.5859 - val_f1macro: 0.3686\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4584 - acc: 0.5416 - f1macro: 0.3503 - val_loss: 0.4609 - val_acc: 0.5391 - val_f1macro: 0.3487\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4505 - acc: 0.5495 - f1macro: 0.3534 - val_loss: 0.4438 - val_acc: 0.5563 - val_f1macro: 0.3563\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4598 - acc: 0.5402 - f1macro: 0.3497 - val_loss: 0.4469 - val_acc: 0.5531 - val_f1macro: 0.3546\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.4541 - acc: 0.5459 - f1macro: 0.3521 - val_loss: 0.4563 - val_acc: 0.5437 - val_f1macro: 0.3514\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.4514 - acc: 0.5486 - f1macro: 0.3535 - val_loss: 0.4297 - val_acc: 0.5703 - val_f1macro: 0.3623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x335745150>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_gen = datagen(data, seq_len, batch_size, 'target', 'train')\n",
    "validation_gen = datagen(data, seq_len, batch_size, 'target', 'valid')\n",
    "\n",
    "model.fit(\n",
    "    training_gen,\n",
    "    validation_data=validation_gen,\n",
    "    epochs=n_epochs, \n",
    "    steps_per_epoch=400, \n",
    "    validation_steps=10, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step\n",
      "accuracy: 0.5421303656597775\n",
      "MAE: 0.4578696343402226\n",
      "F1: 0.7030927835051547\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"target\")\n",
    " \n",
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e134d545e645ca52080ee6eed13987f7920fe874666f42bfb56f6a4194d5976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
