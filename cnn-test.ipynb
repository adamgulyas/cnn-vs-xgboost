{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from os.path import join\n",
    "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from pathlib2 import Path\n",
    "from tensorflow.keras import backend as K, callbacks\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_pos = precision(y_true, y_pred)\n",
    "    recall_pos = recall(y_true, y_pred)\n",
    "    precision_neg = precision((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    recall_neg = recall((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    f_posit = 2 * ((precision_pos * recall_pos) / (precision_pos + recall_pos + K.epsilon()))\n",
    "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
    "\n",
    "    return (f_posit + f_neg) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(file_fir):\n",
    "    try:\n",
    "        df_raw = pd.read_csv(file_fir, index_col='date') # parse_dates=['Date'])\n",
    "    except IOError:\n",
    "        print(\"IO ERROR\")\n",
    "    return df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def costruct_data_warehouse(ROOT_PATH, file_names):\n",
    "    global number_of_stocks\n",
    "    global samples_in_each_stock\n",
    "    global number_feature\n",
    "    global order_stocks\n",
    "    data_warehouse = {}\n",
    "\n",
    "    for stock_file_name in file_names:\n",
    "\n",
    "        file_dir = os.path.join(ROOT_PATH, stock_file_name)\n",
    "        ## Loading Data\n",
    "        # try:\n",
    "        df_raw = load_data(file_dir)\n",
    "        # except ValueError:\n",
    "            # print(\"Couldn't Read {} file\".format(file_dir))\n",
    "\n",
    "        number_of_stocks += 1\n",
    "\n",
    "        data = df_raw\n",
    "        df_name = 'SP500'\n",
    "        order_stocks.append(df_name)\n",
    "        # del data['Name']\n",
    "\n",
    "        target = (data['close'][predict_day:] / data['close'][:-predict_day].values).astype(int)\n",
    "        data = data[:-predict_day]\n",
    "        target.index = data.index\n",
    "        # Becasue of using 200 days Moving Average as one of the features\n",
    "        data = data[200:]\n",
    "        data = data.fillna(0)\n",
    "        data['target'] = target\n",
    "        target = data['target']\n",
    "        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
    "        del data['target']\n",
    "\n",
    "        number_feature = data.shape[1]\n",
    "        samples_in_each_stock = data.shape[0]\n",
    "\n",
    "        train_data = data[data.index < '2020-01-31']\n",
    "        train_data1 = scale(train_data)\n",
    "        # print train_data.shape\n",
    "        train_target1 = target[target.index < '2020-01-31']\n",
    "        train_data = train_data1[:int(0.75 * train_data1.shape[0])]\n",
    "        train_target = train_target1[:int(0.75 * train_target1.shape[0])]\n",
    "\n",
    "        valid_data = scale(train_data1[int(0.75 * train_data1.shape[0]) - seq_len:])\n",
    "        valid_target = train_target1[int(0.75 * train_target1.shape[0]) - seq_len:]\n",
    "\n",
    "        data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
    "        data.index = target.index\n",
    "        test_data = data[data.index >= '2020-01-31']\n",
    "        test_target = target[target.index >= '2020-01-31']\n",
    "\n",
    "        data_warehouse[df_name] = [train_data, train_target, np.array(test_data), np.array(test_target), valid_data,\n",
    "                                   valid_target]\n",
    "\n",
    "    return data_warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cnn_data_sequence_separately(tottal_data, tottal_target, data, target, seque_len):\n",
    "    for index in range(data.shape[0] - seque_len + 1):\n",
    "        tottal_data.append(data[index: index + seque_len])\n",
    "        tottal_target.append(target[index + seque_len - 1])\n",
    "\n",
    "    return tottal_data, tottal_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cnn_data_sequence(data_warehouse, seq_len):\n",
    "    tottal_train_data = []\n",
    "    tottal_train_target = []\n",
    "    tottal_valid_data = []\n",
    "    tottal_valid_target = []\n",
    "    tottal_test_data = []\n",
    "    tottal_test_target = []\n",
    "\n",
    "    for key, value in data_warehouse.items():\n",
    "        tottal_train_data, tottal_train_target = cnn_data_sequence_separately(tottal_train_data, tottal_train_target,\n",
    "                                                                              value[0], value[1], seq_len)\n",
    "        tottal_test_data, tottal_test_target = cnn_data_sequence_separately(tottal_test_data, tottal_test_target,\n",
    "                                                                            value[2], value[3], seq_len)\n",
    "        tottal_valid_data, tottal_valid_target = cnn_data_sequence_separately(tottal_valid_data, tottal_valid_target,\n",
    "                                                                              value[4], value[5], seq_len)\n",
    "\n",
    "    tottal_train_data = np.array(tottal_train_data)\n",
    "    tottal_train_target = np.array(tottal_train_target)\n",
    "    tottal_test_data = np.array(tottal_test_data)\n",
    "    tottal_test_target = np.array(tottal_test_target)\n",
    "    tottal_valid_data = np.array(tottal_valid_data)\n",
    "    tottal_valid_target = np.array(tottal_valid_target)\n",
    "\n",
    "    tottal_train_data = tottal_train_data.reshape(tottal_train_data.shape[0], tottal_train_data.shape[1],\n",
    "                                                  tottal_train_data.shape[2], 1)\n",
    "    tottal_test_data = tottal_test_data.reshape(tottal_test_data.shape[0], tottal_test_data.shape[1],\n",
    "                                                tottal_test_data.shape[2], 1)\n",
    "    tottal_valid_data = tottal_valid_data.reshape(tottal_valid_data.shape[0], tottal_valid_data.shape[1],\n",
    "                                                  tottal_valid_data.shape[2], 1)\n",
    "\n",
    "    return tottal_train_data, tottal_train_target, tottal_test_data, tottal_test_target, tottal_valid_data, tottal_valid_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_acc(model, test_data, test_target):\n",
    "    overall_results = model.predict(test_data)\n",
    "    test_pred = (overall_results > 0.5).astype(int)\n",
    "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
    "                   f1_score(test_pred, test_target, average='macro')]\n",
    "    \n",
    "    predictions.append(test_pred)\n",
    "\n",
    "    return acc_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(data_warehouse, i):\n",
    "    seq_len = 60\n",
    "    epochs = 200\n",
    "    drop = 0.1\n",
    "\n",
    "    global cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target\n",
    "\n",
    "    if i == 1:\n",
    "        print('sequencing ...')\n",
    "        cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = cnn_data_sequence(\n",
    "            data_warehouse, seq_len)\n",
    "\n",
    "    my_file = Path(join(Base_dir,\n",
    "        '2D-models/best-{}-{}-{}-{}-{}.h5'.format(epochs, seq_len, number_filter, drop, i)))\n",
    "    filepath = join(Base_dir, '2D-models/best-{}-{}-{}-{}-{}.h5'.format(epochs, seq_len, number_filter, drop, i))\n",
    "    if my_file.is_file():\n",
    "        print('loading model')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(' fitting model to target')\n",
    "        model = Sequential()\n",
    "        #\n",
    "        # layer 1\n",
    "        model.add(\n",
    "            Conv2D(number_filter[0], (1, number_feature), activation='relu', input_shape=(seq_len, number_feature, 1)))\n",
    "        # layer 2\n",
    "        model.add(Conv2D(number_filter[1], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "        # layer 3\n",
    "        model.add(Conv2D(number_filter[2], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(drop))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(optimizer='Adam', loss='mae', metrics=['acc', f1])\n",
    "\n",
    "        best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n",
    "                                               save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "\n",
    "        model.fit(cnn_train_data, cnn_train_target, epochs=epochs, batch_size=128, verbose=1,\n",
    "                        validation_data=(cnn_valid_data, cnn_valid_target), callbacks=[best_model])\n",
    "    model = load_model(filepath, custom_objects={'f1': f1})\n",
    "\n",
    "    return model, seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence_pre_train(data, target, seque_len):\n",
    "    new_data = []\n",
    "    new_target = []\n",
    "    for index in range(data.shape[0] - seque_len + 1):\n",
    "        new_data.append(data[index: index + seque_len])\n",
    "        new_target.append(target[index + seque_len - 1])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_target = np.array(new_target)\n",
    "\n",
    "    new_data = new_data.reshape(new_data.shape[0], new_data.shape[1], new_data.shape[2], 1)\n",
    "\n",
    "    return new_data, new_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data_warehouse, model, seque_len, order_stocks, cnn_results):\n",
    "    for name in order_stocks:\n",
    "        value = data_warehouse[name]\n",
    "        # train_data, train_target = cnn_data_sequence_pre_train(value[0], value[1], seque_len)\n",
    "        test_data, test_target = cnn_data_sequence_pre_train(value[2], value[3], seque_len)\n",
    "        # valid_data, valid_target = cnn_data_sequence_pre_train(value[4], value[5], seque_len)\n",
    "\n",
    "        cnn_results.append(sklearn_acc(model, test_data, test_target)[2])\n",
    "\n",
    "    return cnn_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_ann(data_warehouse, order_stocks):\n",
    "    cnn_results = []\n",
    "    # dnn_results = []\n",
    "    iterate_no = 4\n",
    "    for i in range(1, iterate_no):\n",
    "        K.clear_session()\n",
    "        print(i)\n",
    "        model, seq_len = train(data_warehouse, i)\n",
    "        # cnn_results, dnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
    "        cnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
    "\n",
    "    cnn_results = np.array(cnn_results)\n",
    "    cnn_results = cnn_results.reshape(iterate_no - 1, len(order_stocks))\n",
    "    cnn_results = pd.DataFrame(cnn_results, columns=order_stocks)\n",
    "    cnn_results = cnn_results.append([cnn_results.mean(), cnn_results.max(), cnn_results.std()], ignore_index=True)\n",
    "    cnn_results.to_csv(join(Base_dir, '2D-models/new results.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data ...\n",
      "number of stocks = \n",
      "1\n",
      "sequencing ...\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniforge3/envs/cnn/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:239: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/Users/adam/miniforge3/envs/cnn/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:239: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "2022-08-08 20:15:36.825759: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step\n",
      "2\n",
      "loading model\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "3\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 20:15:37.038906: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-08 20:15:37.237100: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qs/wmx_ftzd2qb6kytsb50pym_m0000gn/T/ipykernel_5229/3544196779.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cnn_results = cnn_results.append([cnn_results.mean(), cnn_results.max(), cnn_results.std()], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Base_dir = './'\n",
    "TRAIN_ROOT_PATH = join(Base_dir, 'Dataset')\n",
    "train_file_names = os.listdir(join(Base_dir, 'Dataset'))\n",
    "\n",
    "# if moving average = 0 then we have no moving average\n",
    "seq_len = 60\n",
    "moving_average_day = 0\n",
    "number_of_stocks = 0\n",
    "number_feature = 0\n",
    "samples_in_each_stock = 0\n",
    "number_filter = [8, 8, 8]\n",
    "predict_day = 1\n",
    "predictions = []\n",
    "\n",
    "cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = ([] for i in\n",
    "                                                                                                      range(6))\n",
    "\n",
    "\n",
    "\n",
    "print('Loading train data ...')\n",
    "order_stocks = []\n",
    "data_warehouse = costruct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\n",
    "# order_stocks = data_warehouse.keys()\n",
    "\n",
    "print('number of stocks = '), number_of_stocks\n",
    "\n",
    "run_cnn_ann(data_warehouse, order_stocks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e134d545e645ca52080ee6eed13987f7920fe874666f42bfb56f6a4194d5976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
